{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85723,"databundleVersionId":10652996,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/cankszolu/feature-transformation-cleaning?scriptVersionId=216469877\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.330472Z","iopub.execute_input":"2025-01-05T17:19:34.330878Z","iopub.status.idle":"2025-01-05T17:19:34.339276Z","shell.execute_reply.started":"2025-01-05T17:19:34.330851Z","shell.execute_reply":"2025-01-05T17:19:34.338119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Load Data","metadata":{}},{"cell_type":"code","source":"import requests\nimport holidays  # Import the holidays library for country-specific public holiday data\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.340838Z","iopub.execute_input":"2025-01-05T17:19:34.341194Z","iopub.status.idle":"2025-01-05T17:19:34.358216Z","shell.execute_reply.started":"2025-01-05T17:19:34.341166Z","shell.execute_reply":"2025-01-05T17:19:34.356914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Datasets as dataframe\ntrain_data = pd.read_csv(\"/kaggle/input/playground-series-s5e1/train.csv\")\ntest_data = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.360847Z","iopub.execute_input":"2025-01-05T17:19:34.361196Z","iopub.status.idle":"2025-01-05T17:19:34.668809Z","shell.execute_reply.started":"2025-01-05T17:19:34.361167Z","shell.execute_reply":"2025-01-05T17:19:34.667672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.67023Z","iopub.execute_input":"2025-01-05T17:19:34.670641Z","iopub.status.idle":"2025-01-05T17:19:34.674969Z","shell.execute_reply.started":"2025-01-05T17:19:34.670591Z","shell.execute_reply":"2025-01-05T17:19:34.673956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. General Schema","metadata":{}},{"cell_type":"code","source":"# Understand general schema of Train Dataset\ntrain_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.675946Z","iopub.execute_input":"2025-01-05T17:19:34.676255Z","iopub.status.idle":"2025-01-05T17:19:34.74256Z","shell.execute_reply.started":"2025-01-05T17:19:34.67623Z","shell.execute_reply":"2025-01-05T17:19:34.741126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Understand general schema of Test Dataset\ntest_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.743866Z","iopub.execute_input":"2025-01-05T17:19:34.744411Z","iopub.status.idle":"2025-01-05T17:19:34.772971Z","shell.execute_reply.started":"2025-01-05T17:19:34.744369Z","shell.execute_reply":"2025-01-05T17:19:34.771966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's take a look at the first few rows of the training dataset to analyze its structure and \n# understand the data distribution\ntrain_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.774073Z","iopub.execute_input":"2025-01-05T17:19:34.774351Z","iopub.status.idle":"2025-01-05T17:19:34.788074Z","shell.execute_reply.started":"2025-01-05T17:19:34.774328Z","shell.execute_reply":"2025-01-05T17:19:34.787133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Null Values","metadata":{}},{"cell_type":"code","source":"# Null Data per Future\ntrain_data.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.790704Z","iopub.execute_input":"2025-01-05T17:19:34.790981Z","iopub.status.idle":"2025-01-05T17:19:34.851978Z","shell.execute_reply.started":"2025-01-05T17:19:34.790958Z","shell.execute_reply":"2025-01-05T17:19:34.85082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Null Data per Future\ntest_data.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.854105Z","iopub.execute_input":"2025-01-05T17:19:34.854518Z","iopub.status.idle":"2025-01-05T17:19:34.886488Z","shell.execute_reply.started":"2025-01-05T17:19:34.854487Z","shell.execute_reply":"2025-01-05T17:19:34.885381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Handling Null Values\n\nIn the train dataset, **4.01%** of the values are null. We can consider two options here:\n\n1. **Remove the null values:** While this might cause a loss of some information, the percentage of null values is not very high, so it remains a valid option.\n2. **Fill the null values with the mean:** Since our evaluation metric is MAPE, filling null values with the mean is an appropriate choice.\n\nI will choose to fill the null values with the mean.\n","metadata":{}},{"cell_type":"code","source":"#Null values in the 'num_sold' column are filled with the mean to prevent missing data from affecting the model.\ntrain_data['num_sold'] = train_data['num_sold'].fillna(train_data['num_sold'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.887761Z","iopub.execute_input":"2025-01-05T17:19:34.888155Z","iopub.status.idle":"2025-01-05T17:19:34.906373Z","shell.execute_reply.started":"2025-01-05T17:19:34.888101Z","shell.execute_reply":"2025-01-05T17:19:34.905346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.907452Z","iopub.execute_input":"2025-01-05T17:19:34.90783Z","iopub.status.idle":"2025-01-05T17:19:34.960826Z","shell.execute_reply.started":"2025-01-05T17:19:34.907789Z","shell.execute_reply":"2025-01-05T17:19:34.959744Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Date Extraction","metadata":{}},{"cell_type":"code","source":"# Convert the 'date' column to datetime format for easier date-related operations\ntrain_data['date'] = pd.to_datetime(train_data['date'])\ntest_data['date'] = pd.to_datetime(test_data['date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:34.961744Z","iopub.execute_input":"2025-01-05T17:19:34.962043Z","iopub.status.idle":"2025-01-05T17:19:35.012562Z","shell.execute_reply.started":"2025-01-05T17:19:34.96201Z","shell.execute_reply":"2025-01-05T17:19:35.011409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract features\ntrain_data['year'] = train_data['date'].dt.year\ntrain_data['month'] = train_data['date'].dt.month\ntrain_data['day'] = train_data['date'].dt.day\ntrain_data['day_of_week'] = train_data['date'].dt.dayofweek  # Monday = 0, Sunday = 6\ntrain_data['quarter'] = train_data['date'].dt.quarter\ntrain_data['is_weekend'] = train_data['day_of_week'].apply(lambda x: x in [5, 6])\ntrain_data['day_of_year'] = train_data['date'].dt.dayofyear\n\n\ntest_data['year'] = test_data['date'].dt.year\ntest_data['month'] = test_data['date'].dt.month\ntest_data['day'] = test_data['date'].dt.day\ntest_data['day_of_week'] = test_data['date'].dt.dayofweek\ntest_data['quarter'] = test_data['date'].dt.quarter\ntest_data['is_weekend'] = test_data['day_of_week'].apply(lambda x: x in [5, 6])\ntest_data['day_of_year'] = test_data['date'].dt.dayofyear","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:35.013774Z","iopub.execute_input":"2025-01-05T17:19:35.014103Z","iopub.status.idle":"2025-01-05T17:19:35.144408Z","shell.execute_reply.started":"2025-01-05T17:19:35.014076Z","shell.execute_reply":"2025-01-05T17:19:35.143292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to determine the season based on the month\ndef get_season(month):\n    if month in [12, 1, 2]:  # December, January, February -> Winter\n        return 'Winter'\n    elif month in [3, 4, 5]:  # March, April, May -> Spring\n        return 'Spring'\n    elif month in [6, 7, 8]:  # June, July, August -> Summer\n        return 'Summer'\n    else:  # September, October, November -> Autumn\n        return 'Autumn'\n\n# Applying the 'get_season' function to create a 'season' column based on the 'month' column\ntrain_data['season'] = train_data['month'].apply(get_season)  # Add season information to the train dataset\ntest_data['season'] = test_data['month'].apply(get_season)  # Add season information to the test dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:35.14551Z","iopub.execute_input":"2025-01-05T17:19:35.145792Z","iopub.status.idle":"2025-01-05T17:19:35.226178Z","shell.execute_reply.started":"2025-01-05T17:19:35.145769Z","shell.execute_reply":"2025-01-05T17:19:35.225215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Holidays","metadata":{}},{"cell_type":"code","source":"# Mapping countries to their alpha-2 country codes\nalpha2 = dict(zip(np.sort(train_data.country.unique()), ['CA', 'FI', 'IT', 'KE', 'NO', 'SG']))\n\n# Create a dictionary containing public holiday lists for each country (from 2010 to 2019)\nh = {c: holidays.country_holidays(a, years=range(2010, 2020)) for c, a in alpha2.items()}\n\n# Initialize the is_holiday, is_pre_holiday, and last_day_is_holiday columns to 0\ntrain_data['is_holiday'] = 0\ntest_data['is_holiday'] = 0\n\ntrain_data['is_pre_holiday'] = 0\ntest_data['is_pre_holiday'] = 0\n\ntrain_data['last_day_is_holiday'] = 0\ntest_data['last_day_is_holiday'] = 0\n\n# Mark holidays, pre-holiday, and last-day-as-holiday as 1 (True) for each country\nfor c in alpha2:\n    holiday_dates = list(h[c].keys())  # Extract only the holiday dates (as datetime)\n    \n    # For training data\n    train_data.loc[train_data.country == c, 'is_holiday'] = train_data.date.isin(holiday_dates).astype(int)\n    train_data.loc[train_data.country == c, 'is_pre_holiday'] = train_data.date.isin(\n        (pd.to_datetime(holiday_dates) - pd.Timedelta(days=1)).date\n    ).astype(int)\n    train_data.loc[train_data.country == c, 'last_day_is_holiday'] = train_data.date.isin(\n        (pd.to_datetime(holiday_dates) + pd.Timedelta(days=1)).date\n    ).astype(int)\n    \n    # For test data\n    test_data.loc[test_data.country == c, 'is_holiday'] = test_data.date.isin(holiday_dates).astype(int)\n    test_data.loc[test_data.country == c, 'is_pre_holiday'] = test_data.date.isin(\n        (pd.to_datetime(holiday_dates) - pd.Timedelta(days=1)).date\n    ).astype(int)\n    test_data.loc[test_data.country == c, 'last_day_is_holiday'] = test_data.date.isin(\n        (pd.to_datetime(holiday_dates) + pd.Timedelta(days=1)).date\n    ).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:35.2272Z","iopub.execute_input":"2025-01-05T17:19:35.227632Z","iopub.status.idle":"2025-01-05T17:19:35.844807Z","shell.execute_reply.started":"2025-01-05T17:19:35.227551Z","shell.execute_reply":"2025-01-05T17:19:35.843707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the is_holiday, is_pre_holiday, and last_day_is_holiday features to boolean type\ntrain_data['is_holiday'] = train_data['is_holiday'].astype(bool)\ntest_data['is_holiday'] = test_data['is_holiday'].astype(bool)\n\ntrain_data['is_pre_holiday'] = train_data['is_pre_holiday'].astype(bool)\ntest_data['is_pre_holiday'] = test_data['is_pre_holiday'].astype(bool)\n\ntrain_data['last_day_is_holiday'] = train_data['last_day_is_holiday'].astype(bool)\ntest_data['last_day_is_holiday'] = test_data['last_day_is_holiday'].astype(bool)\n\n# Verification: Check the data types after conversion\nprint(train_data[['is_holiday', 'is_pre_holiday', 'last_day_is_holiday']].dtypes)\nprint(test_data[['is_holiday', 'is_pre_holiday', 'last_day_is_holiday']].dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:35.845977Z","iopub.execute_input":"2025-01-05T17:19:35.846309Z","iopub.status.idle":"2025-01-05T17:19:35.863135Z","shell.execute_reply.started":"2025-01-05T17:19:35.846251Z","shell.execute_reply":"2025-01-05T17:19:35.861963Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. GDP","metadata":{}},{"cell_type":"code","source":"# Function to get GDP per capita for a given country and year\ndef get_gdp_per_capita(country, year):\n    alpha3 = {'Canada': 'CAN', 'Finland': 'FIN', 'Italy': 'ITA',\n              'Kenya': 'KEN', 'Norway': 'NOR', 'Singapore': 'SGP'}\n    \n    url = \"https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json\".format(\n        alpha3[country], year)\n    \n    response = requests.get(url).json()\n    return response[1][0]['value']\n\n# Create lists to store country, year, and GDP data\ncountrys = []\nyears = []\ngdps = []\n\n# Get GDP per capita for each country and year (2010-2019)\nfor country in ['Canada', 'Finland', 'Italy', 'Kenya', 'Norway', 'Singapore']:\n    for year in range(2010, 2020):\n        countrys.append(country)\n        years.append(year)\n        gdps.append(get_gdp_per_capita(country, year))\n\n# Create a DataFrame with the GDP data\ngdp_df = pd.DataFrame({\"country\": countrys, \"year\": years, \"gdp\": gdps})\n\n# Merge GDP data with train and test data based on 'country' and 'year'\ntrain_data = pd.merge(train_data, gdp_df, on=['country', 'year'], how='left')\ntest_data = pd.merge(test_data, gdp_df, on=['country', 'year'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:35.864239Z","iopub.execute_input":"2025-01-05T17:19:35.86459Z","iopub.status.idle":"2025-01-05T17:19:53.276167Z","shell.execute_reply.started":"2025-01-05T17:19:35.864564Z","shell.execute_reply":"2025-01-05T17:19:53.274683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the first few rows to confirm the merge\nprint(train_data.head()[['country', 'year', 'gdp']])\nprint(test_data.head()[['country', 'year', 'gdp']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.277667Z","iopub.execute_input":"2025-01-05T17:19:53.27806Z","iopub.status.idle":"2025-01-05T17:19:53.289184Z","shell.execute_reply.started":"2025-01-05T17:19:53.278021Z","shell.execute_reply":"2025-01-05T17:19:53.287922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Date Cycles","metadata":{}},{"cell_type":"code","source":"# Circular encoding for month\ntrain_data['month_sin'] = np.sin(2 * np.pi * train_data['month'] / 12)\ntrain_data['month_cos'] = np.cos(2 * np.pi * train_data['month'] / 12)\ntest_data['month_sin'] = np.sin(2 * np.pi * test_data['month'] / 12)\ntest_data['month_cos'] = np.cos(2 * np.pi * test_data['month'] / 12)\n\n# Normalize day\ntrain_data['day_normalized'] = train_data['day'] / 31\ntest_data['day_normalized'] = test_data['day'] / 31\n\n# Circular encoding for day_of_week\ntrain_data['dow_sin'] = np.sin(2 * np.pi * train_data['day_of_week'] / 7)\ntrain_data['dow_cos'] = np.cos(2 * np.pi * train_data['day_of_week'] / 7)\ntest_data['dow_sin'] = np.sin(2 * np.pi * test_data['day_of_week'] / 7)\ntest_data['dow_cos'] = np.cos(2 * np.pi * test_data['day_of_week'] / 7)\n\n# Applying sine and cosine transformations to the 'day_of_year' feature to capture its cyclical nature\ntrain_data['day_of_year_sin'] = np.sin(2 * np.pi * train_data['day_of_year'] / 365)\ntrain_data['day_of_year_cos'] = np.cos(2 * np.pi * train_data['day_of_year'] / 365)\ntest_data['day_of_year_sin'] = np.sin(2 * np.pi * test_data['day_of_year'] / 365)\ntest_data['day_of_year_cos'] = np.cos(2 * np.pi * test_data['day_of_year'] / 365)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.290381Z","iopub.execute_input":"2025-01-05T17:19:53.290704Z","iopub.status.idle":"2025-01-05T17:19:53.367541Z","shell.execute_reply.started":"2025-01-05T17:19:53.29068Z","shell.execute_reply":"2025-01-05T17:19:53.366647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.368452Z","iopub.execute_input":"2025-01-05T17:19:53.368815Z","iopub.status.idle":"2025-01-05T17:19:53.430296Z","shell.execute_reply.started":"2025-01-05T17:19:53.368772Z","shell.execute_reply":"2025-01-05T17:19:53.429126Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Unique Values","metadata":{}},{"cell_type":"code","source":"# Test: Unique values for each column to determine encoding needs\nfor col in train_data.columns:\n    unique_vals = train_data[col].nunique()\n    print(f\"Column '{col}': {unique_vals} unique values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.431503Z","iopub.execute_input":"2025-01-05T17:19:53.431919Z","iopub.status.idle":"2025-01-05T17:19:53.537158Z","shell.execute_reply.started":"2025-01-05T17:19:53.431874Z","shell.execute_reply":"2025-01-05T17:19:53.536021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identifying and Dropping Unnecessary Columns\ncolumns_to_drop = ['date', 'year', 'month', 'day','day_of_week','day_of_year']\n\ntrain_data = train_data.drop(columns=columns_to_drop, errors='ignore')\ntest_data = test_data.drop(columns=columns_to_drop, errors='ignore')\n\ntrain_data = train_data.drop(columns='id', errors='ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.540667Z","iopub.execute_input":"2025-01-05T17:19:53.540959Z","iopub.status.idle":"2025-01-05T17:19:53.588759Z","shell.execute_reply.started":"2025-01-05T17:19:53.540936Z","shell.execute_reply":"2025-01-05T17:19:53.587773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test: Unique values for each column to determine encoding needs\nfor col in train_data.columns:\n    unique_vals = train_data[col].nunique()\n    print(f\"Column '{col}': {unique_vals} unique values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.589995Z","iopub.execute_input":"2025-01-05T17:19:53.590244Z","iopub.status.idle":"2025-01-05T17:19:53.677639Z","shell.execute_reply.started":"2025-01-05T17:19:53.590223Z","shell.execute_reply":"2025-01-05T17:19:53.676643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Categorical columns to be one-hot encoded\none_hot_columns = ['country', 'store', 'product', 'quarter', 'season']\n\n# Boolean columns to be label encoded or binary encoded\nbinary_columns = ['is_weekend', 'is_holiday', 'is_pre_holiday', 'last_day_is_holiday']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.678605Z","iopub.execute_input":"2025-01-05T17:19:53.678854Z","iopub.status.idle":"2025-01-05T17:19:53.683897Z","shell.execute_reply.started":"2025-01-05T17:19:53.678833Z","shell.execute_reply":"2025-01-05T17:19:53.682729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-Hot Encoding\ntrain_data = pd.get_dummies(train_data, columns=one_hot_columns)\ntest_data = pd.get_dummies(test_data, columns=one_hot_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.685095Z","iopub.execute_input":"2025-01-05T17:19:53.685737Z","iopub.status.idle":"2025-01-05T17:19:53.861889Z","shell.execute_reply.started":"2025-01-05T17:19:53.685695Z","shell.execute_reply":"2025-01-05T17:19:53.86088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in binary_columns:\n    train_data[col] = train_data[col].astype(bool)\n    test_data[col] = test_data[col].astype(bool)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.862961Z","iopub.execute_input":"2025-01-05T17:19:53.863259Z","iopub.status.idle":"2025-01-05T17:19:53.871857Z","shell.execute_reply.started":"2025-01-05T17:19:53.863232Z","shell.execute_reply":"2025-01-05T17:19:53.870853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating checkpoint for next section\ncleaned_train_data = train_data.copy()\ncleaned_test_data = test_data.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.873045Z","iopub.execute_input":"2025-01-05T17:19:53.873426Z","iopub.status.idle":"2025-01-05T17:19:53.903529Z","shell.execute_reply.started":"2025-01-05T17:19:53.873397Z","shell.execute_reply":"2025-01-05T17:19:53.902415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset CSV export\ncleaned_train_data.to_csv('train_data_1.csv', index=False)\n\n# Test dataset CSV export\ncleaned_test_data.to_csv('test_data_1.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:19:53.904801Z","iopub.execute_input":"2025-01-05T17:19:53.905126Z","iopub.status.idle":"2025-01-05T17:19:59.454886Z","shell.execute_reply.started":"2025-01-05T17:19:53.905098Z","shell.execute_reply":"2025-01-05T17:19:59.453794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T17:20:21.215188Z","iopub.execute_input":"2025-01-05T17:20:21.215635Z","iopub.status.idle":"2025-01-05T17:20:21.238516Z","shell.execute_reply.started":"2025-01-05T17:20:21.215604Z","shell.execute_reply":"2025-01-05T17:20:21.237597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}