{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b20611b",
   "metadata": {},
   "source": [
    "# <center>Table of Contents</center>\n",
    "\n",
    "### 1. **Import Libraries**  \n",
    "   - 1A. [Import Required Libraries](#1a-import-required-libraries)  \n",
    "   - 1B. [Create Environment and Test](#1b-create-environment-and-test)  \n",
    "\n",
    "### 2. **Train Model for Normal Version with PPO**  \n",
    "   - 2A. [Train the Model](#2a-train-the-model)  \n",
    "   - 2B. [Save the Model](#2b-save-the-model)  \n",
    "   - 2C. [Evaluate the Model](#2c-evaluate-the-model)  \n",
    "\n",
    "### 3. **Train Model for Hardcore Version with PPO**  \n",
    "   - 3A. [Test the Environment](#3a-test-the-environment)  \n",
    "   - 3B. [Train the Hardcore Model](#3b-train-the-hardcore-model)  \n",
    "   - 3C. [Save the Hardcore Model](#3c-save-the-hardcore-model)  \n",
    "   - 3D. [Evaluate the Hardcore Model](#3d-evaluate-the-hardcore-model)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7c93f",
   "metadata": {},
   "source": [
    "# <center>1. Import Libaries</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6bf97",
   "metadata": {},
   "source": [
    "## 1A) Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38945631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# gymnasium is a modern version of the gym library, used to create and interact with reinforcement learning environments\n",
    "import gymnasium as gym\n",
    "\n",
    "# Import PPO (Proximal Policy Optimization) from stable-baselines3, which is a popular reinforcement learning algorithm\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Import the evaluation function to assess the performance of the trained policy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Import Monitor to log training information such as rewards and episode lengths\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Import utility functions for vectorized environments, normalization, frame stacking, and video recording\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecFrameStack, VecVideoRecorder\n",
    "\n",
    "# Import os for handling directory creation and file paths\n",
    "import os \n",
    "\n",
    "# Import pandas for handling and analyzing data (e.g., log files)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43fc8d",
   "metadata": {},
   "source": [
    "## 1B) Create Env and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BipedalWalker environment with human-rendering mode enabled\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment (start a new episode) - without using seed or options\n",
    "obs = env.reset()\n",
    "\n",
    "# Let the agent take random actions for 1000 steps\n",
    "for _ in range(1000):\n",
    "    # Take a random action sampled from the environment's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward using the chosen action\n",
    "    # The environment returns the new observation (obs), the reward, \n",
    "    # whether the episode is done (done), if it was truncated (truncated), and additional info (info)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is finished (either done or truncated), reset the environment for a new episode\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Close the environment when finished to clean up resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80146f1",
   "metadata": {},
   "source": [
    "# <center>2) Train Model for Normal Version with PPO</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfe8ab",
   "metadata": {},
   "source": [
    "## 2A) Preprocces Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\") #,render_mode = 'rgb_array') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62575b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logs directory and create it if it doesn't exist\n",
    "logs_dir = 'logs'\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Specify the log filename (change this if needed)\n",
    "log_filename = \"\"  # You can change this manually if needed. Default 'monitor.csv', if you add a text it wil be\n",
    "                    #import as (text).monitor.csv\n",
    "\n",
    "# Define the path for the monitor log\n",
    "monitor_log_path = os.path.join(logs_dir, log_filename)\n",
    "\n",
    "# Wrap the environment with Monitor and save logs to the defined path\n",
    "env = Monitor(env, filename=monitor_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment in a DummyVecEnv to enable vectorized operations\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Normalize observations and rewards in the environment\n",
    "# norm_obs: Normalize observations\n",
    "# norm_reward: Normalize rewards\n",
    "# clip_obs: Clip the observation values to prevent outliers\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# Stack the last n_stack observations (here n_stack=4) to provide temporal information to the agent\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = 'videos'\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "env = VecVideoRecorder(env, video_folder, record_video_trigger=lambda x: x % 1000 == 0, video_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28541bb",
   "metadata": {},
   "source": [
    "## 2B) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model with a Multi-Layer Perceptron (MLP) policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795c570",
   "metadata": {},
   "source": [
    "## 2C) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d758bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_bipedalwalker_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab8857",
   "metadata": {},
   "source": [
    "## 2D) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_bipedalwalker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 10 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda44960",
   "metadata": {},
   "source": [
    "# <center>3) Train Model for Hardcore Version with PPO</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ed874",
   "metadata": {},
   "source": [
    "## 3A) Test Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment (start a new episode) - without using seed or options\n",
    "obs = env.reset()\n",
    "\n",
    "# Let the agent take random actions for 1000 steps\n",
    "for _ in range(1000):\n",
    "    # Take a random action sampled from the environment's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward using the chosen action\n",
    "    # The environment returns the new observation (obs), the reward, \n",
    "    # whether the episode is done (done), if it was truncated (truncated), and additional info (info)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is finished (either done or truncated), reset the environment for a new episode\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Close the environment when finished to clean up resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6ed30",
   "metadata": {},
   "source": [
    "## 3B) Preprocces Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True) #,render_mode = 'rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logs directory and create it if it doesn't exist\n",
    "logs_dir = 'logs'\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Specify the log filename (change this if needed)\n",
    "log_filename = \"\"  # You can change this manually if needed. Default 'monitor.csv', if you add a text it wil be\n",
    "                    #import as (text).monitor.csv\n",
    "\n",
    "# Define the path for the monitor log\n",
    "monitor_log_path = os.path.join(logs_dir, log_filename)\n",
    "\n",
    "# Wrap the environment with Monitor and save logs to the defined path\n",
    "env = Monitor(env, filename=monitor_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment in a DummyVecEnv to enable vectorized operations\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Normalize observations and rewards in the environment\n",
    "# norm_obs: Normalize observations\n",
    "# norm_reward: Normalize rewards\n",
    "# clip_obs: Clip the observation values to prevent outliers\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# Stack the last n_stack observations (here n_stack=4) to provide temporal information to the agent\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d37aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the video folder and create it if it doesn't exist\n",
    "video_folder = 'videos'\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "# Wrap the environment with VecVideoRecorder to record videos\n",
    "# The recording is triggered every 1000 steps and each video will be 200 steps long\n",
    "env = VecVideoRecorder(env, video_folder, record_video_trigger=lambda x: x % 1000 == 0, video_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f00ce3",
   "metadata": {},
   "source": [
    "## 3C) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model with a Multi-Layer Perceptron (MLP) policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f6225",
   "metadata": {},
   "source": [
    "## 3D) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3700588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_bipedalwalker_hardcore_3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4cfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e6e40",
   "metadata": {},
   "source": [
    "## 3E) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_bipedalwalker_hardcore_3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63900b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 10 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
