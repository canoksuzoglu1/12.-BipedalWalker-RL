{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b20611b",
   "metadata": {},
   "source": [
    "# <center>Table of Contents</center>\n",
    "\n",
    "### 1. **Import Libraries**  \n",
    "   - 1A. [Import Required Libraries](#1a-import-required-libraries)  \n",
    "   - 1B. [Create Environment and Test](#1b-create-environment-and-test)  \n",
    "\n",
    "### 2. **Train Model for Normal Version with PPO**  \n",
    "   - 2A. [Preprocess Environment](#2a-preprocess-environment)  \n",
    "   - 2B. [Train the Model](#2b-train-the-model)  \n",
    "   - 2C. [Save the Model](#2c-save-the-model)  \n",
    "   - 2D. [Evaluate the Model](#2d-evaluate-the-model)  \n",
    "\n",
    "### 3. **Train Model for Hardcore Version with PPO**  \n",
    "   - 3A. [Test the Environment](#3a-test-the-environment)  \n",
    "   - 3B. [Preprocess Environment](#3b-preprocess-environment)  \n",
    "   - 3C. [Train the Hardcore Model](#3c-train-the-hardcore-model)  \n",
    "   - 3D. [Save the Hardcore Model](#3d-save-the-hardcore-model)  \n",
    "   - 3E. [Evaluate the Hardcore Model](#3e-evaluate-the-hardcore-model)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7c93f",
   "metadata": {},
   "source": [
    "# <center>1. Import Libaries</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6bf97",
   "metadata": {},
   "source": [
    "## 1A) Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38945631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# gymnasium is a modern version of the gym library, used to create and interact with reinforcement learning environments\n",
    "import gymnasium as gym\n",
    "\n",
    "# Import PPO (Proximal Policy Optimization) from stable-baselines3, which is a popular reinforcement learning algorithm\n",
    "from stable_baselines3 import PPO, DQN, DDPG\n",
    "\n",
    "# Import the evaluation function to assess the performance of the trained policy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Import Monitor to log training information such as rewards and episode lengths\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Import utility functions for vectorized environments, normalization, frame stacking, and video recording\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecFrameStack, VecVideoRecorder\n",
    "\n",
    "# Import os for handling directory creation and file paths\n",
    "import os \n",
    "\n",
    "# Import pandas for handling and analyzing data (e.g., log files)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43fc8d",
   "metadata": {},
   "source": [
    "## 1B) Create Env and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BipedalWalker environment with human-rendering mode enabled\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment (start a new episode) - without using seed or options\n",
    "obs = env.reset()\n",
    "\n",
    "# Let the agent take random actions for 1000 steps\n",
    "for _ in range(1000):\n",
    "    # Take a random action sampled from the environment's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward using the chosen action\n",
    "    # The environment returns the new observation (obs), the reward, \n",
    "    # whether the episode is done (done), if it was truncated (truncated), and additional info (info)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is finished (either done or truncated), reset the environment for a new episode\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Close the environment when finished to clean up resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80146f1",
   "metadata": {},
   "source": [
    "# <center>2) Train Model for Normal Version with PPO</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfe8ab",
   "metadata": {},
   "source": [
    "## 2A) Preprocces Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\") #,render_mode = 'rgb_array') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logs directory and create it if it doesn't exist\n",
    "logs_dir = 'logs'\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Specify the log filename (change this if needed)\n",
    "log_filename = \"\"  # You can change this manually if needed. Default 'monitor.csv', if you add a text it wil be\n",
    "                    #import as (text).monitor.csv\n",
    "\n",
    "# Define the path for the monitor log\n",
    "monitor_log_path = os.path.join(logs_dir, log_filename)\n",
    "\n",
    "# Wrap the environment with Monitor and save logs to the defined path\n",
    "env = Monitor(env, filename=monitor_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment in a DummyVecEnv to enable vectorized operations\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Normalize observations and rewards in the environment\n",
    "# norm_obs: Normalize observations\n",
    "# norm_reward: Normalize rewards\n",
    "# clip_obs: Clip the observation values to prevent outliers\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# Stack the last n_stack observations (here n_stack=4) to provide temporal information to the agent\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = 'videos'\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "env = VecVideoRecorder(env, video_folder, record_video_trigger=lambda x: x % 1000 == 0, video_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c4152",
   "metadata": {},
   "source": [
    "## 2B) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model with a Multi-Layer Perceptron (MLP) policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795c570",
   "metadata": {},
   "source": [
    "## 2C) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d758bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_bipedalwalker_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab8857",
   "metadata": {},
   "source": [
    "## 2D) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_bipedalwalker_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 10 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f1af3",
   "metadata": {},
   "source": [
    "Average Reward**: 248.39 ± 112.10\n",
    "  - **Assessment**: This result indicates that the model is performing quite well overall. The average reward suggests that it has developed an effective policy and undergone a successful learning process. The high standard deviation (112.10) indicates that the model achieved significantly higher rewards in some trials while scoring lower in others, implying variability in its responses to different situations. This variability highlights the need for further analysis to understand how the model interacts with its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda44960",
   "metadata": {},
   "source": [
    "# <center>3) Train Model for Hardcore Version with PPO</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ed874",
   "metadata": {},
   "source": [
    "## 3A) Test Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment (start a new episode) - without using seed or options\n",
    "obs = env.reset()\n",
    "\n",
    "# Let the agent take random actions for 1000 steps\n",
    "for _ in range(1000):\n",
    "    # Take a random action sampled from the environment's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward using the chosen action\n",
    "    # The environment returns the new observation (obs), the reward, \n",
    "    # whether the episode is done (done), if it was truncated (truncated), and additional info (info)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is finished (either done or truncated), reset the environment for a new episode\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Close the environment when finished to clean up resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52510fc2",
   "metadata": {},
   "source": [
    "## 3B) Preprocces Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bf2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True) #,render_mode = 'rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logs directory and create it if it doesn't exist\n",
    "logs_dir = 'logs'\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Specify the log filename (change this if needed)\n",
    "log_filename = \"\"  # You can change this manually if needed. Default 'monitor.csv', if you add a text it wil be\n",
    "                    #import as (text).monitor.csv\n",
    "\n",
    "# Define the path for the monitor log\n",
    "monitor_log_path = os.path.join(logs_dir, log_filename)\n",
    "\n",
    "# Wrap the environment with Monitor and save logs to the defined path\n",
    "env = Monitor(env, filename=monitor_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d290ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment in a DummyVecEnv to enable vectorized operations\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Normalize observations and rewards in the environment\n",
    "# norm_obs: Normalize observations\n",
    "# norm_reward: Normalize rewards\n",
    "# clip_obs: Clip the observation values to prevent outliers\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# Stack the last n_stack observations (here n_stack=4) to provide temporal information to the agent\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ad6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the video folder and create it if it doesn't exist\n",
    "video_folder = 'videos'\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "# Wrap the environment with VecVideoRecorder to record videos\n",
    "# The recording is triggered every 1000 steps and each video will be 200 steps long\n",
    "env = VecVideoRecorder(env, video_folder, record_video_trigger=lambda x: x % 1000 == 0, video_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f00ce3",
   "metadata": {},
   "source": [
    "## 3C) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model with a Multi-Layer Perceptron (MLP) policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f6225",
   "metadata": {},
   "source": [
    "## 3D) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3700588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_bipedalwalker_hardcore_3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e6e40",
   "metadata": {},
   "source": [
    "## 3E) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_bipedalwalker_hardcore_3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63900b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 10 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12b832",
   "metadata": {},
   "source": [
    "Average Reward**: -28.23 ± 24.82\n",
    "  - **Assessment**: This result shows that the model is underperforming in the more challenging environment. A negative average reward indicates that the model mostly receives unfavorable feedback and struggles to achieve the target. The lower standard deviation (24.82) suggests less variability in performance, indicating that the model consistently performs poorly under difficult conditions. This may imply that the model requires more training and potentially different hyperparameter settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
