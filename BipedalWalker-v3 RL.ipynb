{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b20611b",
   "metadata": {},
   "source": [
    "# <center>Table of Contents</center>\n",
    "\n",
    "**1. Import Libraries**  \n",
    "   1.1 Import Required Libraries  \n",
    "   1.2 Create Environment and Test\n",
    "\n",
    "**2. Train Model for Normal Version with PPO**  \n",
    "   2.1 Preprocess Environment  \n",
    "   2.2 Train the Model  \n",
    "   2.3 Save the Model  \n",
    "   2.4 Evaluate the Model\n",
    "\n",
    "**3. Train Model for Hardcore Version with PPO**  \n",
    "   3.1 Test the Environment  \n",
    "   3.2 Preprocess Environment  \n",
    "   3.3 Train the Hardcore Model  \n",
    "   3.4 Save the Hardcore Model  \n",
    "   3.5 Evaluate the Hardcore Model\n",
    "\n",
    "**4. 5M Hardcore Training Log Analysis**  \n",
    "   4.1 Data Overview  \n",
    "   4.2 Reward Trend Over Time  \n",
    "   4.3 Episode Length Trend Over Time  \n",
    "   4.4 Correlation Between Reward and Episode Length  \n",
    "   4.5 Episode Length Moving Average  \n",
    "   4.6 Recommendations for Improving Episode Length\n",
    "   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7c93f",
   "metadata": {},
   "source": [
    "# <center>1. Import Libaries</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6bf97",
   "metadata": {},
   "source": [
    "## 1A) Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38945631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# gymnasium is a modern version of the gym library, used to create and interact with reinforcement learning environments\n",
    "import gymnasium as gym\n",
    "\n",
    "# Import PPO (Proximal Policy Optimization) from stable-baselines3, which is a popular reinforcement learning algorithm\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Import the evaluation function to assess the performance of the trained policy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Import pandas for handling and analyzing data (e.g., log files)\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from make_env import make_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43fc8d",
   "metadata": {},
   "source": [
    "## 1B) Create Env and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BipedalWalker environment with human-rendering mode enabled\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment (start a new episode) - without using seed or options\n",
    "obs = env.reset()\n",
    "\n",
    "# Let the agent take random actions for 1000 steps\n",
    "for _ in range(1000):\n",
    "    # Take a random action sampled from the environment's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward using the chosen action\n",
    "    # The environment returns the new observation (obs), the reward, \n",
    "    # whether the episode is done (done), if it was truncated (truncated), and additional info (info)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is finished (either done or truncated), reset the environment for a new episode\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Close the environment when finished to clean up resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80146f1",
   "metadata": {},
   "source": [
    "-----\n",
    "# <center>2. Train Model for Normal Version with PPO</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfe8ab",
   "metadata": {},
   "source": [
    "## 2A) Preprocces Enviorment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e027a1",
   "metadata": {},
   "source": [
    "### Summary of `make_env.py`\n",
    "\n",
    "This function is designed to create and wrap a Gym environment, specifically for the `BipedalWalker-v3` environment, with various configurable features:\n",
    "\n",
    "1. **Environment Creation**:  \n",
    "   - By default, the function creates the `BipedalWalker-v3` environment, but you can specify any Gym environment via the `env_name` parameter.\n",
    "   - The `hardcore` parameter allows enabling or disabling the hardcore mode (`True`/`False`). It defaults to `None`, meaning no hardcore mode unless specified.\n",
    "\n",
    "2. **Observation and Reward Normalization**:  \n",
    "   - The environment is wrapped with `VecNormalize` to normalize observations and rewards, providing more stable training.\n",
    "\n",
    "3. **Frame Stacking for Temporal Information**:  \n",
    "   - The function stacks the last `n_stack` observations (default is 4), which helps the agent to learn from temporal sequences.\n",
    "\n",
    "4. **Video Recording (Optional)**:  \n",
    "   - If `record_video=True`, the environment will record videos every 1000 steps and save them in the specified `video_folder`. The `render_mode` is automatically set to `rgb_array` for recording.\n",
    "\n",
    "5. **Monitor (Enabled by Default)**:  \n",
    "   - The `Monitor` wrapper logs performance metrics such as rewards and episode lengths during training. Logs are saved to the `logs` directory with a timestamp-based filename to avoid overwriting.\n",
    "\n",
    "6. **Vectorized Environment**:  \n",
    "   - The environment is wrapped with `DummyVecEnv` to enable vectorized operations, which are useful for faster training and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7444f",
   "metadata": {},
   "source": [
    "## 2B) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model with a Multi-Layer Perceptron (MLP) policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795c570",
   "metadata": {},
   "source": [
    "## 2C) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d758bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ppo_bipedalwalker_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab8857",
   "metadata": {},
   "source": [
    "## 2D) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_bipedalwalker_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 10 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b2054",
   "metadata": {},
   "source": [
    "**Average Reward**: 248.39 ± 112.10\n",
    "  - **Assessment**: This result indicates that the model is performing quite well overall. The average reward suggests that it has developed an effective policy and undergone a successful learning process. The high standard deviation (112.10) indicates that the model achieved significantly higher rewards in some trials while scoring lower in others, implying variability in its responses to different situations. This variability highlights the need for further analysis to understand how the model interacts with its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda44960",
   "metadata": {},
   "source": [
    "------\n",
    "# <center>3. Train Model for Hardcore Version with PPO</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ed874",
   "metadata": {},
   "source": [
    "## 3A) Test Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment (start a new episode) - without using seed or options\n",
    "obs = env.reset()\n",
    "\n",
    "# Let the agent take random actions for 1000 steps\n",
    "for _ in range(1000):\n",
    "    # Take a random action sampled from the environment's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward using the chosen action\n",
    "    # The environment returns the new observation (obs), the reward, \n",
    "    # whether the episode is done (done), if it was truncated (truncated), and additional info (info)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is finished (either done or truncated), reset the environment for a new episode\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Close the environment when finished to clean up resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0e95c",
   "metadata": {},
   "source": [
    "## 3B) Preprocces Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(hardcore=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f00ce3",
   "metadata": {},
   "source": [
    "## 3C) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model with a Multi-Layer Perceptron (MLP) policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f6225",
   "metadata": {},
   "source": [
    "## 3D) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3700588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ppo_bipedalwalker_hardcore_3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e6e40",
   "metadata": {},
   "source": [
    "## 3E) Evaluate Model 3M Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_bipedalwalker_hardcore_3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63900b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 10 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a254acf",
   "metadata": {},
   "source": [
    "**Average Reward**: -28.23 ± 24.82\n",
    "  - **Assessment**: This result shows that the model is underperforming in the more challenging environment. A negative average reward indicates that the model mostly receives unfavorable feedback and struggles to achieve the target. The lower standard deviation (24.82) suggests less variability in performance, indicating that the model consistently performs poorly under difficult conditions. This may imply that the model requires more training and potentially different hyperparameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c176f0",
   "metadata": {},
   "source": [
    "## 3F) Evaluate Model 5M Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38caf0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_bipedalwalker_hardcore_5M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d487d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98888d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 100 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d70eba",
   "metadata": {},
   "source": [
    "**Average Reward**: -10.66 ± 3.91  \n",
    "- **Assessment**: This result indicates that the model is not performing well in the environment, as evidenced by the negative average reward. A negative score suggests that the agent primarily receives penalties, reflecting its struggle to reach the desired outcomes. The standard deviation of 3.91 indicates relatively low variability in performance, meaning the model consistently underperforms rather than showing sporadic successes. This suggests that the model may benefit from further training and adjustments in hyperparameters to improve its learning effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbb82d",
   "metadata": {},
   "source": [
    "-----\n",
    "# <center>4. 5m Hardcore Training Log Analysis </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08519928",
   "metadata": {},
   "source": [
    "This section provides an in-depth analysis of the 5m hardcore training logs. The analysis focuses on key metrics such as reward, episode length, and their correlation, with visualizations to help interpret the results effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd5be0",
   "metadata": {},
   "source": [
    "## 4A) Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda6a3b",
   "metadata": {},
   "source": [
    "The training log contains three key columns:\n",
    "- `reward`: The reward obtained by the agent in each episode.\n",
    "- `episode_length`: The length (number of steps) of each episode.\n",
    "- `time`: The time elapsed during the training process.\n",
    "\n",
    "We start by loading the data and cleaning it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('logs/5m_hardcore.monitor.csv', skiprows=1)\n",
    "data.columns = ['reward', 'episode_length', 'time']\n",
    "data_clean = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b80ea",
   "metadata": {},
   "source": [
    "## 4B) Reward Trend Over Time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a02350",
   "metadata": {},
   "source": [
    "In the first step, we visualize how the reward evolves over time during training. This helps in understanding how well the agent is performing over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833daa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_clean['time'], data_clean['reward'], label='Reward')\n",
    "plt.title('Reward Over Time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95855fb3",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "The reward fluctuates significantly over time but shows a general stabilization trend. This suggests that the agent may have reached a steady learning phase where its performance remains stable with minor variations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2712d0",
   "metadata": {},
   "source": [
    "## 4C) Episode Length Trend Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150c33f",
   "metadata": {},
   "source": [
    "Next, we examine how the episode length changes over time. This metric helps understand how long the agent survives or performs in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode length over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_clean['time'], data_clean['episode_length'], label='Episode Length', color='orange')\n",
    "plt.title('Episode Length Over Time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75504fca",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "The episode length tends to remain relatively high throughout the training, with occasional dips. This indicates that the agent consistently completes longer episodes, which could mean it is learning to survive longer in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0921cc5",
   "metadata": {},
   "source": [
    "## 4D) Correlation Between Reward and Episode Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281efd8",
   "metadata": {},
   "source": [
    "A key question is whether there is a correlation between the reward and the episode length. To investigate this, we calculate the correlation coefficient between these two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between reward and episode length\n",
    "correlation = data_clean['reward'].corr(data_clean['episode_length'])\n",
    "print(f'Correlation between reward and episode length: {correlation:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fccebd",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "The calculated correlation is 0.89, which indicates a strong positive correlation. This means that as the episode length increases, the reward also tends to increase. Essentially, the longer the agent survives, the more reward it earns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63711b62",
   "metadata": {},
   "source": [
    "## 4E) Episode Length Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc00c53",
   "metadata": {},
   "source": [
    "To smooth out the episode length data and observe longer-term trends, we use a moving average with a window size of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average of episode length\n",
    "window_size = 50\n",
    "data_clean['episode_length_ma'] = data_clean['episode_length'].rolling(window=window_size).mean()\n",
    "\n",
    "# Plot episode length with moving average\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_clean['time'], data_clean['episode_length'], label='Episode Length', color='orange')\n",
    "plt.plot(data_clean['time'], data_clean['episode_length_ma'], label=f'Moving Average ({window_size} windows)', color='blue')\n",
    "plt.title('Episode Length Over Time with Moving Average')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffc146",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "The moving average reveals that the episode length has a slight upward trend over time, indicating that the agent may be gradually learning to perform longer episodes as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73629b9",
   "metadata": {},
   "source": [
    "## 4F) Recommendations for Improving Episode Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d239c5",
   "metadata": {},
   "source": [
    "Based on the analysis, here are some strategies to potentially increase the episode length and improve agent performance:\n",
    "\n",
    "**1. Adjust Learning Rate:** Consider lowering the learning rate to allow for more gradual improvements.\n",
    "\n",
    "**2. Modify Reward Function:** Adjust the reward structure to incentivize the agent for surviving longer in each episode.\n",
    "\n",
    "**3. Increase Exploration:** Encourage more exploration by adjusting the epsilon in ε-greedy policies or employing curiosity-driven methods.\n",
    "\n",
    "**4. Extend Training Duration:** Increasing the number of timesteps during training may allow the agent to learn better strategies for longer survival.\n",
    "\n",
    "**5. Use Experience Replay:** Implementing experience replay could help the agent learn from past episodes and improve over time.\n",
    "\n",
    "By following these recommendations, the agent’s performance could be enhanced, leading to longer episode durations and improved rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7a6dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This write-up includes Markdown text for Jupyter, along with code snippets for generating visualizations and insights. It summarizes key findings such as reward trends, episode length behavior, and actionable steps to improve training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
