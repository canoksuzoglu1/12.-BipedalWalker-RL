{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b20611b",
   "metadata": {},
   "source": [
    "# <center>**Table of Contents**</center>\n",
    "\n",
    "**0. About Bipedal Walker**\n",
    "\n",
    "**1. Import Libraries**\n",
    "   - 1.A Import Required Libraries\n",
    "   - 1.B Create Environment and Test\n",
    "\n",
    "**2. Train Model for Normal Version with PPO**\n",
    "   - 2.A Preprocess Environment\n",
    "   - 2.B Train the Model\n",
    "   - 2.C Save the Model\n",
    "   - 2.D Evaluate the Model\n",
    "\n",
    "**3. Train Model for Hardcore Version with PPO**\n",
    "   - 3.A Test the Environment\n",
    "   - 3.B Preprocess Environment\n",
    "   - 3.C Train the Hardcore Model\n",
    "   - 3.D Save the Hardcore Model\n",
    "   - 3.E Evaluate 3M Model\n",
    "   - 3.F Observe 3M Model in Human Render Mode\n",
    "   - 3.G Evaluate 5M Model\n",
    "   - 3.H Observe 5M Model in Human Render Mode\n",
    "   - 3.I Evaluate 7M Model\n",
    "\n",
    "**4. 5M Hardcore Training Log Analysis**\n",
    "   - 4.1 Data Overview\n",
    "   - 4.2 Reward Trend Over Time\n",
    "   - 4.3 Episode Length Trend Over Time\n",
    "   - 4.4 Correlation Between Reward and Episode Length\n",
    "   - 4.5 Episode Length Moving Average\n",
    "   - 4.6 Recommendations for Improving Episode Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d3bfb",
   "metadata": {},
   "source": [
    "# <center>0. About Bidepal Walker</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98c0a1",
   "metadata": {},
   "source": [
    "## Bipedal Walker (Box2D)\n",
    "\n",
    "The **Bipedal Walker** environment simulates a bipedal robot with 4 joints and 2 legs, where the goal is to walk across rugged, uneven terrain. The task requires the agent to balance and coordinate its movements effectively over a variety of surfaces.\n",
    "\n",
    "### Observation Space:\n",
    "- The observation space includes 24 continuous values, which provide detailed information on:\n",
    "  - Hull angle and angular velocity\n",
    "  - Horizontal and vertical speed\n",
    "  - Joint angles and speeds for both legs\n",
    "  - 10 LIDAR readings that measure the distances to the terrain below\n",
    "\n",
    "### Action Space:\n",
    "- The action space consists of 4 continuous values in the range \\([-1, 1]\\), each controlling the torque applied to the robot's joints:\n",
    "  - Hip and knee joints for both legs\n",
    "  \n",
    "### Rewards:\n",
    "- **Positive Rewards**: For forward movement and maintaining balance.\n",
    "- **Negative Rewards**: Penalties are given for applying excessive torque to the joints and for falling.\n",
    "\n",
    "### Termination:\n",
    "- The episode ends if the robot falls or if the maximum number of steps (1600 in normal mode or 2000 in hardcore mode) is reached.\n",
    "\n",
    "The environment is designed to challenge both learning algorithms and the agent's ability to handle continuous control tasks in varying terrains.\n",
    "\n",
    "For more information, refer to the [Bipedal Walker documentation](https://gymnasium.farama.org/environments/box2d/bipedal_walker/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7c93f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <center>1. Import Libaries</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6bf97",
   "metadata": {},
   "source": [
    "## 1A) Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38945631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for handling and analyzing data (e.g., log files)\n",
    "import pandas as pd\n",
    "\n",
    "# Import matplotlib for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import necessary utility functions from env_utils\n",
    "from env_utils import make_env, observe_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43fc8d",
   "metadata": {},
   "source": [
    "## 1B) Create Env and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BipedalWalker environment with human-rendering mode enabled\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment (start a new episode) - without using seed or options\n",
    "obs = env.reset()\n",
    "\n",
    "# Let the agent take random actions for 1000 steps\n",
    "for _ in range(1000):\n",
    "    # Take a random action sampled from the environment's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward using the chosen action\n",
    "    # The environment returns the new observation (obs), the reward, \n",
    "    # whether the episode is done (done), if it was truncated (truncated), and additional info (info)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is finished (either done or truncated), reset the environment for a new episode\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Close the environment when finished to clean up resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80146f1",
   "metadata": {},
   "source": [
    "-----\n",
    "# <center>2. Train Model for Normal Version with PPO</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfe8ab",
   "metadata": {},
   "source": [
    "## 2A) Preprocces Enviorment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f325dc",
   "metadata": {},
   "source": [
    "### Summary of `make_env.py`\n",
    "\n",
    "This function is designed to create and wrap a Gym environment, specifically for the `BipedalWalker-v3` environment, with various configurable features:\n",
    "\n",
    "1. **Environment Creation**:  \n",
    "   - By default, the function creates the `BipedalWalker-v3` environment, but you can specify any Gym environment via the `env_name` parameter.\n",
    "   - The `hardcore` parameter allows enabling or disabling the hardcore mode (`True`/`False`). It defaults to `None`, meaning no hardcore mode unless specified.\n",
    "\n",
    "2. **Observation and Reward Normalization**:  \n",
    "   - The environment is wrapped with `VecNormalize` to normalize observations and rewards, providing more stable training.\n",
    "\n",
    "3. **Frame Stacking for Temporal Information**:  \n",
    "   - The function stacks the last `n_stack` observations (default is 4), which helps the agent to learn from temporal sequences.\n",
    "\n",
    "4. **Video Recording (Optional)**:  \n",
    "   - If `record_video=True`, the environment will record videos every 1000 steps and save them in the specified `video_folder`. The `render_mode` is automatically set to `rgb_array` for recording.\n",
    "\n",
    "5. **Monitor (Enabled by Default)**:  \n",
    "   - The `Monitor` wrapper logs performance metrics such as rewards and episode lengths during training. Logs are saved to the `logs` directory with a timestamp-based filename to avoid overwriting.\n",
    "\n",
    "6. **Vectorized Environment**:  \n",
    "   - The environment is wrapped with `DummyVecEnv` to enable vectorized operations, which are useful for faster training and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20cb5c",
   "metadata": {},
   "source": [
    "## 2B) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model with a Multi-Layer Perceptron (MLP) policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795c570",
   "metadata": {},
   "source": [
    "## 2C) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d758bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ppo_bipedalwalker_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab8857",
   "metadata": {},
   "source": [
    "## 2D) Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_bipedalwalker_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 10 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222552bf",
   "metadata": {},
   "source": [
    "**Average Reward**: 248.39 ± 112.10\n",
    "  - **Assessment**: This result indicates that the model is performing quite well overall. The average reward suggests that it has developed an effective policy and undergone a successful learning process. The high standard deviation (112.10) indicates that the model achieved significantly higher rewards in some trials while scoring lower in others, implying variability in its responses to different situations. This variability highlights the need for further analysis to understand how the model interacts with its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb279f8",
   "metadata": {},
   "source": [
    "## 2E) Observe Model in Human Render Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344232e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_model(model_path = 'models/ppo_bipedalwalker_1M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda44960",
   "metadata": {},
   "source": [
    "------\n",
    "# <center>3. Train Model for Hardcore Version with PPO</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ed874",
   "metadata": {},
   "source": [
    "## 3A) Test Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment (start a new episode) - without using seed or options\n",
    "obs = env.reset()\n",
    "\n",
    "# Let the agent take random actions for 1000 steps\n",
    "for _ in range(1000):\n",
    "    # Take a random action sampled from the environment's action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment forward using the chosen action\n",
    "    # The environment returns the new observation (obs), the reward, \n",
    "    # whether the episode is done (done), if it was truncated (truncated), and additional info (info)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the episode is finished (either done or truncated), reset the environment for a new episode\n",
    "    if done or truncated:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Close the environment when finished to clean up resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b99f6e",
   "metadata": {},
   "source": [
    "## 3B) Preprocces Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(hardcore=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f00ce3",
   "metadata": {},
   "source": [
    "## 3C) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PPO model with a Multi-Layer Perceptron (MLP) policy\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f6225",
   "metadata": {},
   "source": [
    "## 3D) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3700588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ppo_bipedalwalker_hardcore_3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e6e40",
   "metadata": {},
   "source": [
    "## 3E) Evaluate Model 3M Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_bipedalwalker_hardcore_3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", hardcore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63900b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 10 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947143d",
   "metadata": {},
   "source": [
    "**Average Reward**: -28.23 ± 24.82\n",
    "  - **Assessment**: This result shows that the model is underperforming in the more challenging environment. A negative average reward indicates that the model mostly receives unfavorable feedback and struggles to achieve the target. The lower standard deviation (24.82) suggests less variability in performance, indicating that the model consistently performs poorly under difficult conditions. This may imply that the model requires more training and potentially different hyperparameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfcf74",
   "metadata": {},
   "source": [
    "## 3F) Observe 3M Model in Human Render Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_model(model_path = 'models/ppo_bipedalwalker_hardcore_3M', hardcore = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18f4b9",
   "metadata": {},
   "source": [
    "## 3G) Evaluate Model 5M Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00690273",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc13e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_bipedalwalker_hardcore_5M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"BipedalWalker-v3\", hardcore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 100 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ea433",
   "metadata": {},
   "source": [
    "**Average Reward**: -10.66 ± 3.91  \n",
    "- **Assessment**: This result indicates that the model is not performing well in the environment, as evidenced by the negative average reward. A negative score suggests that the agent primarily receives penalties, reflecting its struggle to reach the desired outcomes. The standard deviation of 3.91 indicates relatively low variability in performance, meaning the model consistently underperforms rather than showing sporadic successes. This suggests that the model may benefit from further training and adjustments in hyperparameters to improve its learning effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47717536",
   "metadata": {},
   "source": [
    "## 3H) Observe 5M Model in Human Render Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38325226",
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_model(model_path = 'models/ppo_bipedalwalker_hardcore_7M', hardcore = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14454416",
   "metadata": {},
   "source": [
    "## 3I) Evaluate Model 7M Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd505081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/ppo_bipedalwalker_hardcore_7M\")\n",
    "env = make_env(\"BipedalWalker-v3\", hardcore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60be75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (e.g., over 100 episodes)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"Average reward: {mean_reward} ± {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcb57f",
   "metadata": {},
   "source": [
    "-----\n",
    "# <center>4. 5m Hardcore Training Log Analysis </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383ec6d",
   "metadata": {},
   "source": [
    "This section provides an in-depth analysis of the 5m hardcore training logs. The analysis focuses on key metrics such as reward, episode length, and their correlation, with visualizations to help interpret the results effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a0636",
   "metadata": {},
   "source": [
    "## 4A) Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480c2b1",
   "metadata": {},
   "source": [
    "The training log contains three key columns:\n",
    "- `reward`: The reward obtained by the agent in each episode.\n",
    "- `episode_length`: The length (number of steps) of each episode.\n",
    "- `time`: The time elapsed during the training process.\n",
    "\n",
    "We start by loading the data and cleaning it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf34c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('logs/5m_hardcore.monitor.csv', skiprows=1)\n",
    "data.columns = ['reward', 'episode_length', 'time']\n",
    "data_clean = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2785f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c231541",
   "metadata": {},
   "source": [
    "## 4B) Reward Trend Over Time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99058e00",
   "metadata": {},
   "source": [
    "In the first step, we visualize how the reward evolves over time during training. This helps in understanding how well the agent is performing over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_clean['time'], data_clean['reward'], label='Reward')\n",
    "plt.title('Reward Over Time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8a221",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "The reward fluctuates significantly over time but shows a general stabilization trend. This suggests that the agent may have reached a steady learning phase where its performance remains stable with minor variations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06941c3",
   "metadata": {},
   "source": [
    "## 4C) Episode Length Trend Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e550a",
   "metadata": {},
   "source": [
    "Next, we examine how the episode length changes over time. This metric helps understand how long the agent survives or performs in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2607bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode length over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_clean['time'], data_clean['episode_length'], label='Episode Length', color='orange')\n",
    "plt.title('Episode Length Over Time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b30fb2",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "The episode length tends to remain relatively high throughout the training, with occasional dips. This indicates that the agent consistently completes longer episodes, which could mean it is learning to survive longer in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad48fef",
   "metadata": {},
   "source": [
    "## 4D) Correlation Between Reward and Episode Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631d3ca",
   "metadata": {},
   "source": [
    "A key question is whether there is a correlation between the reward and the episode length. To investigate this, we calculate the correlation coefficient between these two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between reward and episode length\n",
    "correlation = data_clean['reward'].corr(data_clean['episode_length'])\n",
    "print(f'Correlation between reward and episode length: {correlation:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d3b16",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "The calculated correlation is 0.89, which indicates a strong positive correlation. This means that as the episode length increases, the reward also tends to increase. Essentially, the longer the agent survives, the more reward it earns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058b8e1",
   "metadata": {},
   "source": [
    "## 4E) Episode Length Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ada25a",
   "metadata": {},
   "source": [
    "To smooth out the episode length data and observe longer-term trends, we use a moving average with a window size of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average of episode length\n",
    "window_size = 50\n",
    "data_clean['episode_length_ma'] = data_clean['episode_length'].rolling(window=window_size).mean()\n",
    "\n",
    "# Plot episode length with moving average\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_clean['time'], data_clean['episode_length'], label='Episode Length', color='orange')\n",
    "plt.plot(data_clean['time'], data_clean['episode_length_ma'], label=f'Moving Average ({window_size} windows)', color='blue')\n",
    "plt.title('Episode Length Over Time with Moving Average')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d870eff",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "The moving average reveals that the episode length has a slight upward trend over time, indicating that the agent may be gradually learning to perform longer episodes as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfdca52",
   "metadata": {},
   "source": [
    "## 4F) Recommendations for Improving Episode Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc203990",
   "metadata": {},
   "source": [
    "Based on the analysis, here are some strategies to potentially increase the episode length and improve agent performance:\n",
    "\n",
    "**1. Adjust Learning Rate:** Consider lowering the learning rate to allow for more gradual improvements.\n",
    "\n",
    "**2. Modify Reward Function:** Adjust the reward structure to incentivize the agent for surviving longer in each episode.\n",
    "\n",
    "**3. Increase Exploration:** Encourage more exploration by adjusting the epsilon in ε-greedy policies or employing curiosity-driven methods.\n",
    "\n",
    "**4. Extend Training Duration:** Increasing the number of timesteps during training may allow the agent to learn better strategies for longer survival.\n",
    "\n",
    "**5. Use Experience Replay:** Implementing experience replay could help the agent learn from past episodes and improve over time.\n",
    "\n",
    "By following these recommendations, the agent’s performance could be enhanced, leading to longer episode durations and improved rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09f7b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This write-up includes Markdown text for Jupyter, along with code snippets for generating visualizations and insights. It summarizes key findings such as reward trends, episode length behavior, and actionable steps to improve training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
